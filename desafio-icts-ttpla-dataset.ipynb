{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Desafio Prático de Machine Learning\n",
        "***\n",
        "> Solução feita por Alysson Machado de Oliveira Barbosa"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instruções do Desafio\n",
        "***"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Escreva um algoritmo em Python que realize o treinamento de um modelo para a detecção de torres de energia e linhas de energia em imagens.\n",
        "2. Siga as instruções do repositório para extração de rótulos no formato [COCO dataset](https://cocodataset.org/#home). Utilize imagens de tamanho 640 x 360.\n",
        "3. Respeite a divisão da base de dados quanto aos conjuntos de treino, validação e teste.\n",
        "4. Atente-se a um código limpo, organizado, documentado e com ideias claras da solução proposta.\n",
        "5. Caso mais de um modelo seja produzido, comente sobre os resultados obtidos, explicitando qual deles funcionou melhor e o porquê.\n",
        "6. Na falta de uma GPU para o treinamento, recomenda-se o uso do Kaggle ou do Google Colab"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![imagem-capa](imagens-ilustrativas/imagem-capa.jpeg)\n",
        "> Segmentação por instância de torres de energia e linhas de força."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Abordagem do Problema\n",
        "***"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* ```Arquitetura Utilizada```: **YoloV8**.\n",
        "\n",
        "A YOLOv8 é um modelo de rede neural utilizado para resolver problemas de segmentação por instância e detecção de objetos de forma simultânea. YOLO, que significa \"*You Only Look Once*\", é uma abordagem de detecção de objetos em tempo real que opera diretamente em uma imagem inteira, em vez de dividir a imagem em regiões menores, como outros métodos. O YOLOv8 é uma versão aprimorada que utiliza uma arquitetura de rede neural convolucional profunda para detectar e segmentar objetos em uma imagem, atribuindo rótulos e coordenadas de caixa delimitadora a cada objeto encontrado. Essa abordagem oferece uma detecção rápida e eficiente de objetos em tempo real, sendo amplamente utilizado em aplicações de vigilância, automação industrial, veículos autônomos e outras áreas onde a detecção de objetos é essencial.\n",
        "\n",
        "* ```Servidor para treinar o modelo```: **Google Colab (GPU Tesla T4)**."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Análise do Dataset\n",
        "****"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* ```Dataset utilizado```: [TTPLA: An Aerial-Image Dataset for Detection and Segmentation of Transmission Towers and Power Lines](https://github.com/R3ab/ttpla_dataset).\n",
        "\n",
        "O dataset TTPLA contém imagens aéreas de torres e linhas de transmissão. Cada imagem possui anotações de segmentação poligonal dos objetos em análise, organizada em arquivos ```.json``` no formato [Coco Dataset](https://cocodataset.org/#home)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparação dos Dados\n",
        "***"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Redimensionamento das imagens\n",
        "\n",
        "O dataset original foi obtido através do Google Drive, disponibilizado pelos criadores no GitHub, na qual o mesmo pode ser obtidos [clicando aqui](https://drive.google.com/uc?export=download&confirm=no_antivirus&id=1Yz59yXCiPKS0_X4K3x9mW22NLnxjvrr0). Esse dataset foi adicionada a pasta ```dataset-original```. \n",
        "\n",
        "Em seguida, o script localizado em ```scripts/resize_image_and_annotation.py``` foi modificado para que fosse possível redimensionar as imagens do dataset para um tamanho de 640x360.   \n",
        "\n",
        "A partir do script ```scripts/resize_image_and_annotation.py```, foi utilizado o seguinte código na linha de comando para que uma nova pasta fosse gerada com os códigos redimensionados:\n",
        "\n",
        "```\n",
        "python resize_image_and_annotation-final.py -t dataset-original/\n",
        "```\n",
        "\n",
        "A nova pasta com as imagens redimensionadas foi renomeada para ```dados-redimensionados/```.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Remoção de dados rotulados em vazio\n",
        "\n",
        "Para evitar que problemas fossem obtidos durante a etapa de treinamento, imagens que contêm regiões sem rótulo definido foram removidas. Foi utilizado o script localizado em ```scripts/remove_void.py``` para resolver esse problema, através do seguinte código de comando:\n",
        "\n",
        "```\n",
        "python remove_void.py -t dados-redimensionados/\n",
        "```\n",
        "\n",
        "A nova pasta com os arquivos ```.json``` corrigidos foi renomeada para ```jsons-corrigidos/```."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Divisão do dataset em treinamento/validação/teste\n",
        "\n",
        "Utilizando a sugestão de divisão dos dados, armazenados em arquivos ```.txt``` em estratos de treinamento, validação e teste, em uma aproximação respectiva de 70%, 10% e 20%, respectivamente, localizada na pasta ```guia-estratificacao-arquivos/```, o script ```scripts/split_jsons.py``` foi utilizado para resolver esse problema. O seguinte código de comando foi utilizado:\n",
        "\n",
        "```\n",
        "python split_jsons.py -t jsons-corrigidos/\n",
        "```\n",
        "\n",
        "A nova pasta com os arquivos ```.json``` estratificados foi renomeada para ```jsons-estratificados/```."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Remoção de imagens com rotulações problemáticas\n",
        "\n",
        "Para verificar se todas as imagens estão de fato no padrão [Coco Dataset](https://cocodataset.org/#home), para serem utilizadas no padrão de treinamento de uma arquitetura Yolo, por exemplo, o seguinte script localizado em ```scripts/labelme2coco_2.py``` foi utilizado para realizar essa verificação. O procedimento consistiu em modificar esse script definido pelos criadores do dataset, que tinha o objetivo primário de alterar os rótulos para o padrão da arquitetura ```YoloAct```, para que ele retornasse o caminho relativo da imagem que fazia o código não conseguir realizar essa conversão, de modo que o mesmo não fosse compilado devido a uma mensagem de erro. Dessa forma, todas as imagens problemáticas (em torno de 16 imagens) foram excluídas.\n",
        "\n",
        "```\n",
        "python labelme2coco_2.py jsons-estratificados/jsons-treinamento\n",
        "python labelme2coco_2.py jsons-estratificados/jsons-validacao\n",
        "python labelme2coco_2.py jsons-estratificados/jsons-teste\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5. Upload do dataset no RoboFlow para ser utilizado no Google Colab\n",
        "\n",
        "Devido a limitações técnicas em meu computador pessoal, o Google Colab foi o servidor escolhido para treinar o modelo de detecção e segmentação por instância no YoloV8 utilizando o dataset pré-processado TTPLA. No Google Colab, uma GPU do tipos Tesla T4 foi utilizada.\n",
        "\n",
        "O dataset pré-processado foi colocado no RoboFlow por três motivos:\n",
        "1. Facilidade para baixá-lo utilizando código em Python pelo Google Colab;\n",
        "2. Fácil reuso toda vez que uma nova sessão no servidor do Google Colab for criada;\n",
        "3. Conversão fácil do dataset organizado no estilo do [Coco Dataset](https://cocodataset.org/#home) para o estilo do YoloV8;\n",
        "\n",
        "O dataset pré-processado para cumprir com os requisitos do ICTS podem ser baixados pelo link: \n",
        "* [https://universe.roboflow.com/alysson-machado/ttpla-qx1sw](https://universe.roboflow.com/alysson-machado/ttpla-qx1sw)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![marcacao-exemplo1](imagens-ilustrativas/marcacao-exemplo1.jpeg)\n",
        "> Imagen marcada e pré-processada no formato solicitado pelo desafio do ICTS."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![marcacao-exemplo2](imagens-ilustrativas/marcacao-exemplo2.jpeg)\n",
        "> Imagen marcada e pré-processada no formato solicitado pelo desafio do ICTS."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparação do Ambiente de Treinamento\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8cDtxLIBHgQ",
        "outputId": "e0dd7392-6ea2-4c38-96f1-87ecc7c4f9e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Jun 18 18:04:39 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# verificando a disponibilidade de GPU do Google Colab\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjpPg4mGKc1v",
        "outputId": "c0eb070c-4874-450b-8218-3673c600dd50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "# verificando o diretório atual de trabalho do Google Colab\n",
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdSMcABDNKW-",
        "outputId": "d052e1a0-7abd-43ab-ab6d-634b1bd6aa08"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.28 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 24.1/78.2 GB disk)\n"
          ]
        }
      ],
      "source": [
        "# instalando pelo pip a arquitetura da YoloV8\n",
        "!pip install ultralytics==8.0.28\n",
        "# limpando a célula do Jupyter com as informações de instalação do pacote\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "# verificando o status da instalação do pacote e compatibilidade com o servidor\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VOEYrlBoP9-E"
      },
      "outputs": [],
      "source": [
        "# importando a arquitetura YoloV8 do pacote ultralytics\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import display, Image"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baixando o Dataset Pré-processado no Servidor do Jupyter\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSd93ZJzZZKt",
        "outputId": "ca4548ac-095b-40e0-d3e5-42d0af1f8968"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/datasets\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Dependency ultralytics<=8.0.20 is required but found version=8.0.28, to fix: `pip install ultralytics<=8.0.20`\n",
            "Downloading Dataset Version Zip in TTPLA-1 to yolov8: 99% [189743104 / 190902459] bytes"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Dataset Version Zip to TTPLA-1 in yolov8:: 100%|██████████| 4780/4780 [00:02<00:00, 2204.34it/s]\n"
          ]
        }
      ],
      "source": [
        "# criando um diretório para baixar o dataset importado do roboflow\n",
        "!mkdir {HOME}/datasets\n",
        "%cd {HOME}/datasets\n",
        "\n",
        "# instalando os pacotes do roboflow para fazer a conexão com a API\n",
        "!pip install roboflow --quiet\n",
        "from roboflow import Roboflow\n",
        "\n",
        "# conectando com a API do RoboFlow para baixar o dataset pré-processado \n",
        "# OBS: por motivos didáticos, deixarei explicita a chave de acesso para a API \n",
        "# de download do dataset que fiz upload para o site, mas normalmente eu não constumo\n",
        "# upar essa informação sensível em repositórios abertos\n",
        "rf = Roboflow(api_key=\"xhUBoRw9sdlgzMz1Zw0Y\")\n",
        "project = rf.workspace(\"alysson-machado\").project(\"ttpla-qx1sw\")\n",
        "dataset = project.version(1).download(\"yolov8\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YUjFBKKqXa-u"
      },
      "source": [
        "## Treinando a Arquitetura da YoloV8\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2YkphuiaE7_",
        "outputId": "55d4f399-07e5-48ef-ac18-617b1f45dc82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-seg.pt to yolov8x-seg.pt...\n",
            "100% 137M/137M [00:07<00:00, 18.3MB/s]\n",
            "Ultralytics YOLOv8.0.28 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=segment, mode=train, model=yolov8x-seg.pt, data=/content/datasets/TTPLA-1/data.yaml, epochs=30, patience=50, batch=16, imgsz=640, save=True, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, save_dir=runs/segment/train\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 113MB/s]\n",
            "2023-06-18 18:08:28.815171: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-18 18:08:29.658492: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Overriding model.yaml nc=80 with nc=6\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      2320  ultralytics.nn.modules.Conv                  [3, 80, 3, 2]                 \n",
            "  1                  -1  1    115520  ultralytics.nn.modules.Conv                  [80, 160, 3, 2]               \n",
            "  2                  -1  3    436800  ultralytics.nn.modules.C2f                   [160, 160, 3, True]           \n",
            "  3                  -1  1    461440  ultralytics.nn.modules.Conv                  [160, 320, 3, 2]              \n",
            "  4                  -1  6   3281920  ultralytics.nn.modules.C2f                   [320, 320, 6, True]           \n",
            "  5                  -1  1   1844480  ultralytics.nn.modules.Conv                  [320, 640, 3, 2]              \n",
            "  6                  -1  6  13117440  ultralytics.nn.modules.C2f                   [640, 640, 6, True]           \n",
            "  7                  -1  1   3687680  ultralytics.nn.modules.Conv                  [640, 640, 3, 2]              \n",
            "  8                  -1  3   6969600  ultralytics.nn.modules.C2f                   [640, 640, 3, True]           \n",
            "  9                  -1  1   1025920  ultralytics.nn.modules.SPPF                  [640, 640, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 12                  -1  3   7379200  ultralytics.nn.modules.C2f                   [1280, 640, 3]                \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 15                  -1  3   1948800  ultralytics.nn.modules.C2f                   [960, 320, 3]                 \n",
            " 16                  -1  1    922240  ultralytics.nn.modules.Conv                  [320, 320, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 18                  -1  3   7174400  ultralytics.nn.modules.C2f                   [960, 640, 3]                 \n",
            " 19                  -1  1   3687680  ultralytics.nn.modules.Conv                  [640, 640, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 21                  -1  3   7379200  ultralytics.nn.modules.C2f                   [1280, 640, 3]                \n",
            " 22        [15, 18, 21]  1  12321986  ultralytics.nn.modules.Segment               [6, 32, 320, [320, 640, 640]] \n",
            "YOLOv8x-seg summary: 401 layers, 71756626 parameters, 71756610 gradients, 344.5 GFLOPs\n",
            "\n",
            "Transferred 651/657 items from pretrained weights\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 106 weight(decay=0.0), 117 weight(decay=0.0005), 116 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/TTPLA-1/train/labels... 1669 images, 562 backgrounds, 0 corrupt: 100% 1669/1669 [00:00<00:00, 1973.20it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/TTPLA-1/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/TTPLA-1/valid/labels... 238 images, 80 backgrounds, 0 corrupt: 100% 238/238 [00:00<00:00, 1902.90it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/TTPLA-1/valid/labels.cache\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/segment/train\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/30      14.4G      1.494      2.565      2.658      1.579         26        640: 100% 105/105 [02:43<00:00,  1.56s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:11<00:00,  1.44s/it]\n",
            "                   all        238       1134      0.298      0.188      0.101     0.0699      0.273       0.14     0.0586     0.0281\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/30      15.3G      1.093      1.684       1.69      1.256         46        640: 100% 105/105 [02:39<00:00,  1.52s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:11<00:00,  1.42s/it]\n",
            "                   all        238       1134      0.549      0.197      0.143     0.0908      0.578      0.132      0.107      0.051\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/30      15.3G      1.156       1.69      1.711      1.287         42        640: 100% 105/105 [02:41<00:00,  1.54s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.33s/it]\n",
            "                   all        238       1134      0.386      0.269      0.171      0.117      0.366      0.221      0.123       0.07\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/30      15.3G      1.209      1.755      1.676      1.316         24        640: 100% 105/105 [02:42<00:00,  1.55s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:11<00:00,  1.39s/it]\n",
            "                   all        238       1134      0.454      0.165      0.124     0.0815      0.345      0.218      0.103     0.0451\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/30      15.3G      1.233      1.729      1.683      1.327         12        640: 100% 105/105 [02:43<00:00,  1.56s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.36s/it]\n",
            "                   all        238       1134      0.376      0.167      0.112     0.0682      0.317      0.121     0.0546     0.0189\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/30      15.3G      1.222      1.756      1.622      1.324         60        640: 100% 105/105 [02:41<00:00,  1.54s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.34s/it]\n",
            "                   all        238       1134      0.515      0.213      0.196      0.113      0.446      0.169      0.134     0.0738\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/30      15.3G      1.175      1.683      1.558      1.305         50        640: 100% 105/105 [02:42<00:00,  1.55s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.35s/it]\n",
            "                   all        238       1134      0.438      0.233      0.202      0.131      0.389      0.168      0.129     0.0745\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/30      15.3G      1.124      1.658      1.492       1.28         58        640: 100% 105/105 [02:42<00:00,  1.55s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.35s/it]\n",
            "                   all        238       1134      0.684     0.0763     0.0758       0.05      0.666     0.0536     0.0467     0.0219\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/30      15.3G      1.113      1.649      1.451      1.269         44        640: 100% 105/105 [02:42<00:00,  1.55s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:11<00:00,  1.41s/it]\n",
            "                   all        238       1134      0.529      0.265      0.232      0.151      0.457      0.201      0.154     0.0805\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/30      15.3G      1.103      1.645      1.437      1.264         47        640: 100% 105/105 [02:41<00:00,  1.53s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.31s/it]\n",
            "                   all        238       1134      0.548      0.294      0.279      0.184      0.454      0.255      0.203      0.103\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/30      15.3G      1.038      1.605      1.344      1.232         41        640: 100% 105/105 [02:42<00:00,  1.55s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.34s/it]\n",
            "                   all        238       1134      0.533      0.243      0.241      0.171      0.575      0.205      0.196     0.0988\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/30      15.3G      1.042      1.583      1.315      1.232         59        640: 100% 105/105 [02:42<00:00,  1.55s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.34s/it]\n",
            "                   all        238       1134      0.592      0.271      0.271      0.184      0.543      0.219      0.203      0.107\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/30      15.3G      1.029      1.542       1.32      1.227         71        640: 100% 105/105 [02:42<00:00,  1.55s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:11<00:00,  1.38s/it]\n",
            "                   all        238       1134      0.522      0.334      0.298      0.208      0.485      0.262      0.222      0.127\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/30      15.3G     0.9962      1.492      1.306      1.208         58        640: 100% 105/105 [02:41<00:00,  1.54s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.33s/it]\n",
            "                   all        238       1134      0.556       0.33      0.305      0.222      0.642      0.251      0.249      0.136\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/30      15.3G     0.9848      1.473      1.252      1.192         39        640: 100% 105/105 [02:42<00:00,  1.54s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.35s/it]\n",
            "                   all        238       1134      0.601       0.33      0.323       0.23      0.567      0.289      0.259      0.149\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/30      15.3G     0.9418      1.422      1.185      1.166         50        640: 100% 105/105 [02:41<00:00,  1.54s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.32s/it]\n",
            "                   all        238       1134      0.572      0.344      0.358      0.242      0.596       0.27      0.273      0.143\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/30      15.3G     0.9473      1.448      1.168      1.171         32        640: 100% 105/105 [02:43<00:00,  1.55s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.34s/it]\n",
            "                   all        238       1134      0.681      0.341      0.373      0.262      0.719      0.247      0.272      0.159\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/30      15.3G     0.9058      1.433      1.125      1.146         45        640: 100% 105/105 [02:42<00:00,  1.55s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.32s/it]\n",
            "                   all        238       1134      0.438      0.386      0.406      0.279      0.394      0.347      0.328       0.18\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/30      15.3G     0.8978      1.424      1.106      1.146         25        640: 100% 105/105 [02:40<00:00,  1.53s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.34s/it]\n",
            "                   all        238       1134      0.615      0.354       0.37      0.258      0.438      0.275      0.269       0.15\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/30      15.3G     0.8665      1.369      1.063      1.128         55        640: 100% 105/105 [02:42<00:00,  1.54s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.32s/it]\n",
            "                   all        238       1134      0.507      0.368      0.393      0.261      0.433      0.324      0.304      0.163\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      21/30      15.3G      1.009     0.9244      1.548      1.191          1        640: 100% 105/105 [02:34<00:00,  1.47s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.31s/it]\n",
            "                   all        238       1134      0.509       0.18        0.2      0.134      0.315       0.13      0.124     0.0624\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      22/30      15.3G     0.9514     0.9049       1.12      1.186         13        640: 100% 105/105 [02:33<00:00,  1.46s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.32s/it]\n",
            "                   all        238       1134      0.533       0.39      0.407      0.299      0.707      0.293      0.323      0.191\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      23/30      15.3G     0.9474     0.8879      1.166      1.178         47        640: 100% 105/105 [02:32<00:00,  1.45s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.33s/it]\n",
            "                   all        238       1134      0.614      0.415       0.43      0.303      0.386      0.324      0.326      0.182\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      24/30      15.3G     0.9045     0.8622      1.066      1.155         14        640: 100% 105/105 [02:32<00:00,  1.45s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.36s/it]\n",
            "                   all        238       1134       0.61      0.364      0.415      0.293      0.481      0.334       0.34      0.195\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      25/30      15.3G     0.8718     0.8375      1.068      1.136          1        640: 100% 105/105 [02:33<00:00,  1.46s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.33s/it]\n",
            "                   all        238       1134      0.666      0.386      0.438      0.317      0.828      0.286       0.36      0.204\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      26/30      15.3G     0.8636     0.8427      1.035       1.14         13        640: 100% 105/105 [02:32<00:00,  1.45s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.33s/it]\n",
            "                   all        238       1134      0.631      0.365      0.439      0.325      0.502      0.312      0.348      0.203\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      27/30      15.3G     0.8254     0.8105      0.965      1.119         20        640: 100% 105/105 [02:32<00:00,  1.46s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.32s/it]\n",
            "                   all        238       1134      0.719      0.427      0.489      0.355      0.701      0.385      0.409      0.219\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      28/30      15.3G     0.8008     0.8008     0.9013      1.093         12        640: 100% 105/105 [02:33<00:00,  1.46s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.31s/it]\n",
            "                   all        238       1134      0.671      0.458       0.49       0.36      0.567      0.361      0.404       0.23\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      29/30      15.3G     0.7665     0.7971     0.8644      1.081         20        640: 100% 105/105 [02:32<00:00,  1.45s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:10<00:00,  1.32s/it]\n",
            "                   all        238       1134      0.635      0.484        0.5      0.363      0.474      0.415      0.424      0.236\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      30/30      15.3G     0.7546     0.7935     0.8498      1.076         33        640: 100% 105/105 [02:33<00:00,  1.46s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:17<00:00,  2.15s/it]\n",
            "                   all        238       1134      0.851      0.417      0.505      0.359      0.619       0.37      0.413      0.236\n",
            "\n",
            "30 epochs completed in 1.457 hours.\n",
            "Optimizer stripped from runs/segment/train/weights/last.pt, 143.9MB\n",
            "Optimizer stripped from runs/segment/train/weights/best.pt, 143.9MB\n",
            "\n",
            "Validating runs/segment/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.28 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8x-seg summary (fused): 295 layers, 71726434 parameters, 0 gradients, 343.7 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 8/8 [00:16<00:00,  2.09s/it]\n",
            "                   all        238       1134      0.635      0.484        0.5      0.363      0.469      0.416      0.424      0.237\n",
            "                 cable        238        998      0.654       0.63      0.644      0.505      0.527      0.494      0.416      0.197\n",
            "         tower_lattice        238         32      0.697      0.648      0.659      0.552      0.692      0.633      0.649      0.477\n",
            "          tower_tucohy        238         36      0.736      0.543      0.619      0.419      0.729        0.5      0.649      0.322\n",
            "          tower_wooden        238         31      0.404      0.484      0.429      0.232      0.397      0.452      0.402      0.188\n",
            "                  void        238         37      0.682      0.117       0.15      0.111          0          0     0.0016   0.000964\n",
            "Speed: 2.4ms pre-process, 31.5ms inference, 0.0ms loss, 4.1ms post-process per image\n",
            "Results saved to \u001b[1mruns/segment/train\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%cd {HOME}\n",
        "# detalhes da linha de comando:\n",
        "# 'task=segment' -> informar a arquitetura que os dados de treinamento estão organizados no padrão de segmentação\n",
        "# 'mode=train' -> informar que a arquitetura pré-treinada vai ser retreinada utilizando transferência de aprendizado\n",
        "# 'model=yolov8x-seg.pt' -> informar que a arquitetura a ser retreinada é do tipo x, da mais robusta\n",
        "# 'data={dataset.location}/data.yaml' -> passa a informação da localização do dataset formatado\n",
        "# 'epochs=30' -> define a quantidade de épocas de treinamento \n",
        "# 'imgsz=640' -> informa que a arquitetura a ser retreinada é para o padrão 640x360\n",
        "\n",
        "!yolo task=segment mode=train model=yolov8x-seg.pt data={dataset.location}/data.yaml epochs=30 imgsz=640"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O modelo treinado foi adicionado a pasta ```modelo-treinado/```."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Análise do Modelo Treinado\n",
        "***"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Diversos gráficos para os dados de treinamento e validação foram adicionadas a pasta ```metricas-avaliativas/``` para melhor análise. Nesses gráficos, é contemplado em mais detalhes a performace da capacidade de detecção de objetos e segmentação, utilizando métricas avaliativos como precisão, sensibilidade e F1-score."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![metricas](metricas-avaliativas/dados-treinamento/results.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Com base nos resultados obtidos, o modelo treinado apresenta alguns pontos fortes e pontos fracos. No lado positivo, o modelo alcança uma precisão (```precision```) relativamente alta para detecção de torres (0.85) e uma precisão moderada para detecção de linhas de energia (0.61). Além disso, a perda de caixa (```box loss```) e a perda de segmentação (```seg loss```) são consistentemente reduzidas durante o treinamento, sugerindo que o modelo está aprendendo a localizar e segmentar com eficiência esses elementos nas imagens aéreas."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por outro lado, existem algumas áreas que podem ser melhoradas. Primeiramente, a perda de classificação (```cls loss```) e a perda de deslocamento de características (```dfl loss```) estão relativamente altas em comparação com as outras perdas. Isso pode indicar que o modelo está tendo dificuldade em classificar corretamente as torres e linhas de energia, bem como em realizar ajustes finos de localização. Além disso, as métricas de recall (```recall```) para ambos os objetos são baixas, indicando que o modelo está deixando de detectar algumas instâncias de torres e linhas de energia."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para melhorar o modelo, algumas sugestões podem ser consideradas. Primeiramente, é recomendável aumentar o tamanho do conjunto de treinamento, adicionando mais exemplos de torres e linhas de energia em diferentes contextos e condições de iluminação. Isso ajudará o modelo a generalizar melhor para casos mais diversos. Além disso, ajustar os hiperparâmetros do modelo, como a taxa de aprendizado e o tamanho do lote, pode levar a melhorias adicionais no desempenho. Também pode ser útil explorar técnicas de aumento de dados, como rotações e espelhamentos, para fornecer ao modelo uma maior variedade de exemplos durante o treinamento. Por fim, realizar uma análise mais detalhada dos falsos positivos e falsos negativos durante a validação pode ser útil para melhorar a qualidade geral das detecções."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MScstfHhArr",
        "outputId": "3c4982fc-fc50-454b-9b6b-52b4b545b1b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "args.yaml\t\t\t\t\t    train_batch0.jpg\n",
            "BoxF1_curve.png\t\t\t\t\t    train_batch1.jpg\n",
            "BoxP_curve.png\t\t\t\t\t    train_batch2100.jpg\n",
            "BoxPR_curve.png\t\t\t\t\t    train_batch2101.jpg\n",
            "BoxR_curve.png\t\t\t\t\t    train_batch2102.jpg\n",
            "confusion_matrix.png\t\t\t\t    train_batch2.jpg\n",
            "events.out.tfevents.1687111713.32e5e3524560.1712.0  val_batch0_labels.jpg\n",
            "MaskF1_curve.png\t\t\t\t    val_batch0_pred.jpg\n",
            "MaskP_curve.png\t\t\t\t\t    val_batch1_labels.jpg\n",
            "MaskPR_curve.png\t\t\t\t    val_batch1_pred.jpg\n",
            "MaskR_curve.png\t\t\t\t\t    val_batch2_labels.jpg\n",
            "results.csv\t\t\t\t\t    val_batch2_pred.jpg\n",
            "results.png\t\t\t\t\t    weights\n"
          ]
        }
      ],
      "source": [
        "# listando as imagens com as métricas quantitativas e qualitativas obtidas a respeito do treinamento\n",
        "# mais detalhes dessas métricas avaliativas estão disponíveis na pasta 'metricas-avaliativas/'\n",
        "!ls {HOME}/runs/segment/train/"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i4eASbcWkQBq"
      },
      "source": [
        "## Inferências nos Dados de Teste e em Vídeos\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE-or9MIpOll",
        "outputId": "335de2c8-7652-457f-8547-4dd24180a07e"
      },
      "outputs": [],
      "source": [
        "# realizando uma predição em um vídeo de exemplo baixado do YouTube\n",
        "%cd {HOME}\n",
        "!yolo task=segment mode=predict model={HOME}/runs/segment/train/weights/best.pt conf=0.25 source='/content/video-example.mp4' save=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFP3CZKUDjMz",
        "outputId": "8c283ed2-2c5b-479b-ac99-ca85225b8adc"
      },
      "outputs": [],
      "source": [
        "# realizando uma predição em um vídeo de exemplo baixado do YouTube\n",
        "%cd {HOME}\n",
        "!yolo task=segment mode=predict model={HOME}/runs/segment/train/weights/best.pt conf=0.25 source='/content/video-example2.mp4' save=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ew1tqUnDvdg",
        "outputId": "3e1ebeff-ca54-436d-fb7e-da074ff6736c"
      },
      "outputs": [],
      "source": [
        "# realizando uma predição em um vídeo de exemplo baixado do YouTube\n",
        "%cd {HOME}\n",
        "!yolo task=segment mode=predict model={HOME}/runs/segment/train/weights/best.pt conf=0.25 source='/content/video-example3.mp4' save=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wjc1ctZykYuf",
        "outputId": "7768fe31-8bcc-466b-ab5f-b6a56b144c58"
      },
      "outputs": [],
      "source": [
        "# realizando a predição nos dados selecionados como teste\n",
        "%cd {HOME}\n",
        "!yolo task=segment mode=predict model={HOME}/runs/segment/train/weights/best.pt conf=0.25 source={dataset.location}/test/images save=true"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Vídeos de Exemplo\n",
        "\n",
        "A seguir, é possível analisar os resultados obtidos com a predição feita nos três vídeos selecionados. Os vídeos estão localizados na pasta ```resultados-videos/``` e também os coloquei no YouTube para maior facilidade de análise:\n",
        "\n",
        "* Primeiro Vídeo: [https://youtu.be/QyB5Sq-Rt_A](https://youtu.be/QyB5Sq-Rt_A)\n",
        "* Segundo Vídeo: [https://youtu.be/5booA-m-5Ns](https://youtu.be/5booA-m-5Ns)\n",
        "* Terceiro Vídeo: [https://youtu.be/KNTggwp_2WI](https://youtu.be/KNTggwp_2WI)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Imagens do Conjunto de Teste\n",
        "\n",
        "A seguir, é possível visualizar o resultado de algumas imagens disponíveis no conjunto de teste. Mais imagens podem ser visualizadas na pasta ```resultados-teste/```.\n",
        "\n",
        "|     |     |\n",
        "| ----------- | ----------- |\n",
        "| ![Imagem 1](resultados-teste/13_00263.rf.31bff92f2ab0cea9e341544b6749bbdc.jpg)  | ![Imagem 2](resultados-teste/16_3450.rf.2bdf90684515866182ec4c154579e982.jpg)  |\n",
        "| ![Imagem 3](resultados-teste/19_00563.rf.00e9332d2073d6b790cfcdc05693d95d.jpg)  | ![Imagem 4](resultados-teste/19_00934.rf.54bf10bc96f149536fb23f9e7fa65df5.jpg)  |\n",
        "| ![Imagem 5](resultados-teste/31_7785.rf.8a631e630477fea86c5d5ea8d32d1d45.jpg)  | ![Imagem 6](resultados-teste/33_7020.rf.e2d7ef6a43ad603718d17ef6e69df2a9.jpg)  |\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
